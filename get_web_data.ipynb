{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e931a503-4e98-4816-8361-aee831a43c4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip3 install pytest-playwright\n",
    "# !playwright install  # install playwright browsers by running !playwright install from Jupyter\n",
    "# !pip3 install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "069db812-0193-46a6-a988-3239b455cf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "\n",
    "# Imports\n",
    "import sys\n",
    "import os\n",
    "import platform\n",
    "from os.path import exists\n",
    "from watermark import watermark\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "from bs4 import BeautifulSoup   #\n",
    "from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeout  #\n",
    "from typing import List\n",
    "\n",
    "import time   #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "765a992e-e84b-498e-907a-d71733d05cda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Platform: macOS-13.0.1-arm64-arm-64bit\n",
      "Python 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:12:31) [Clang 14.0.6 ]\n",
      "Last updated: 2023-05-29T16:38:46.065923-05:00\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.10.10\n",
      "IPython version      : 8.13.2\n",
      "\n",
      "Compiler    : Clang 14.0.6 \n",
      "OS          : Darwin\n",
      "Release     : 22.1.0\n",
      "Machine     : arm64\n",
      "Processor   : arm\n",
      "CPU cores   : 10\n",
      "Architecture: 64bit\n",
      "\n",
      "platform  : 1.0.8\n",
      "sys       : 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:12:31) [Clang 14.0.6 ]\n",
      "tensorflow: 2.12.0\n",
      "pandas    : 2.0.1\n",
      "\n",
      "TensorFlow version: 2.12.0\n",
      "Is MPS available?: False\n",
      "Using device: /device:CPU:0\n",
      "Metal device set to: Apple M1 Max\n"
     ]
    }
   ],
   "source": [
    "# Report Technologies\n",
    "print(f'Python Platform: {platform.platform()}')\n",
    "print(f'Python {sys.version}')\n",
    "print(watermark())\n",
    "print(watermark(iversions=True, globals_=globals()))\n",
    "\n",
    "mps_available = tf.config.list_logical_devices(\"MPS\")\n",
    "device = \"/device:MPS:0\" if len(mps_available) > 0 else \"/device:CPU:0\"\n",
    "print(f\"TensorFlow version: {tf.__version__}\\nIs MPS available?: {len(mps_available) > 0}\\nUsing device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b0301dc-79f4-4c01-8f0b-9279910b96ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2019, 2020, 2021, 2022]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEASONS = list(range(2019,2023))   #\n",
    "\n",
    "SEASONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7660a739-16ac-45b3-b1dc-1708965f40e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# BLOCK ONE\n",
    "#\n",
    "\n",
    "# Define the parent directory for data\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "# Define the subdirectory for storing standings data\n",
    "STANDINGS_DIR = os.path.join(DATA_DIR, \"standings\")\n",
    "\n",
    "# Define the subdirectory for storing game scores data\n",
    "SCORES_DIR = os.path.join(DATA_DIR, \"scores\")\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9ad8412-cf06-4624-880a-9c7528222761",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# BLOCK TWO\n",
    "#\n",
    "\n",
    "\n",
    "# function to obtain html\n",
    "async def get_html(url: str, selector: str, sleep: int = 5, retries: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    An asynchronous function to get HTML code from a website given a URL and a CSS selector for the content.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL to the webpage to be scraped.\n",
    "        selector (str): The CSS selector for the content to be extracted.\n",
    "        sleep (int, optional): The amount of time (in seconds) to sleep between retries. Defaults to 5.\n",
    "        retries (int, optional): The number of times to retry if the content extraction fails. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        str: The HTML content of the selected element.\n",
    "    \"\"\"\n",
    "    html = None\n",
    "    for i in range(1, retries+1):   # default retries up to 3 times in event of failure\n",
    "        time.sleep(sleep * i)       # pause for 'sleep' seconds to avoid website detection; and website ban;  5, 10, 15 second pauses\n",
    "        try:\n",
    "            async with async_playwright() as p:     # initialize a Playwright instance for web scraping\n",
    "                browser = await p.chromium.launch() # launch a Chromium browser instance\n",
    "                page = await browser.new_page()     # create a new page object in the browser\n",
    "                await page.goto(url)                # navigate to the URL in the page object\n",
    "                print(await page.title())           # print the page title (for debugging)\n",
    "                html = await page.inner_html(selector) # extract the HTML content from the page object using the CSS selector\n",
    "        except PlaywrightTimeout:   # if a timeout error occurs, print a message and try again\n",
    "            print(f'Timeout error on {url}')\n",
    "            continue\n",
    "        else:   # if the HTML content is successfully obtained, break the loop and return the content\n",
    "            break\n",
    "    return html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0e62d92-719b-4701-b1d2-a3889551abb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# BLOCK THREE\n",
    "#\n",
    "\n",
    "# Define an asynchronous function to scrape the standings for a given season\n",
    "async def scrape_season(season: str) -> None:\n",
    "    url = f\"https://www.basketball-reference.com/leagues/NBA_{season}_games.html\"   # Set the URL to the page containing game schedules for the given season\n",
    "    html = await get_html(url, \"#content .filter\")   # Use the `get_html` function to get the page HTML, passing in the URL and CSS selector\n",
    "    \n",
    "    soup = BeautifulSoup(html)   # Use BeautifulSoup to parse the HTML\n",
    "    links = soup.find_all(\"a\")   # Find all the links on the page\n",
    "    standings_pages = [f\"https://www.basketball-reference.com{l['href']}\" for l in links]   # Create a list of URLs for each standings page by appending the relative path to the base URL\n",
    "    \n",
    "    for url in standings_pages:   # Loop over each standings page URL\n",
    "        save_path = os.path.join(STANDINGS_DIR, url.split(\"/\")[-1])   # Set the save path by joining the `STANDINGS_DIR` path with the last part of the URL\n",
    "        if os.path.exists(save_path):   # If the file already exists, skip to the next URL\n",
    "            continue\n",
    "        \n",
    "        html = await get_html(url, \"#all_schedule\")   # Use the `get_html` function to get the page HTML for the current URL\n",
    "        with open(save_path, \"w+\") as f:   # Open the file at the save path with write access\n",
    "            f.write(html)   # Write the HTML to the file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b57fdf99-f56d-4772-8d43-840e2a1a5461",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.basketball-reference.com/leagues/NBA_2021_games.html'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "season = 2021\n",
    "url = f'https://www.basketball-reference.com/leagues/NBA_{season}_games.html'\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cccd5a08-1bdc-45a7-9f91-94986c4ff044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# html = await get_html(url, '#content.filter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dca921-e876-4d8a-a7de-fdd29685ea67",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timeout error on https://www.basketball-reference.com/leagues/NBA_2019_games.html\n",
      "Timeout error on https://www.basketball-reference.com/leagues/NBA_2019_games.html\n",
      "2018-19 NBA Schedule | Basketball-Reference.com\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# BLOCK FOUR\n",
    "#\n",
    "\n",
    "# loop through each season in the list of seasons\n",
    "for season in SEASONS:          \n",
    "    await scrape_season(season) # asynchronously scrape the data for that season\n",
    "    \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27b3b3b-e6a1-4544-8296-c72e154c95bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# BLOCK FIVE\n",
    "#\n",
    "\n",
    "# get a list of all the files in the standings directory\n",
    "standings_files = os.listdir(STANDINGS_DIR)   \n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b80158c-04f1-4a00-b629-db6b6bd7e0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# BLOCK SIX\n",
    "#\n",
    "\n",
    "# function to scrape individual game box scores\n",
    "async def scrape_game(standings_file: str) -> None:\n",
    "    with open(standings_file, 'r') as f:   # open the standings file and read its contents\n",
    "        html = f.read()                    # store the contents of the standings file in the 'html' variable\n",
    "\n",
    "    soup = BeautifulSoup(html)             # create a BeautifulSoup object from the 'html' variable\n",
    "    links = soup.find_all(\"a\")             # find all the links in the BeautifulSoup object\n",
    "    hrefs = [l.get('href') for l in links] # extract the href attribute of each link and store in a list called 'hrefs'\n",
    "    box_scores = [f\"https://www.basketball-reference.com{l}\" for l in hrefs if l and \"boxscore\" in l and '.html' in l] # create a list of all the box score URLs\n",
    "\n",
    "    for url in box_scores:  # loop through each box score URL\n",
    "        save_path = os.path.join(SCORES_DIR, url.split(\"/\")[-1]) # create a save path for the file based on its URL\n",
    "        if os.path.exists(save_path): # if the file already exists, skip it\n",
    "            continue\n",
    "\n",
    "        html = await get_html(url, \"#content\") # scrape the HTML from the box score URL using the 'get_html' function\n",
    "        if not html:  # if no HTML is returned, skip to the next URL\n",
    "            continue\n",
    "        with open(save_path, \"w+\") as f: # open the save path file in write mode\n",
    "            f.write(html) # write the HTML to the file\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cc0fc9-65c5-4572-9d8e-1e4e7aaac8da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# BLOCK SEVEN\n",
    "#\n",
    "\n",
    "# loop over each season in the list of seasons\n",
    "for season in SEASONS:  # SEASONS is assumed to be a list of integers\n",
    "    files = [s for s in standings_files if str(season) in s]  # filter the list of standings files to only include files for the current season\n",
    "    \n",
    "    for f in files:  # loop over each file for the current season\n",
    "        filepath = os.path.join(STANDINGS_DIR, f)  # create the full file path for the current file\n",
    "        \n",
    "        await scrape_game(filepath)  # scrape the games for the season in the current file\n",
    "        \n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
